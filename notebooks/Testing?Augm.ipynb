{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You try to forget everything good, and remember everything bad.\\nyou just try to forget everything good, and remember everything bad.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lemarx/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/lemarx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21')]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = tada.split('\\n')[0]\n",
    "# seq = re.sub(r\"[^a-zA-Z0-9]+\", ' ', test)\n",
    "# seq = list(set(seq.split()))\n",
    "\n",
    "# candidates = [word for word in seq if len(word) >= 3]\n",
    "# #words = nltk.word_tokenize(seq)\n",
    "# pos_tags = nltk.pos_tag(candidates)\n",
    "\n",
    "\n",
    "# word,tag = pos_tags[0]\n",
    "\n",
    "# wn.synsets('good', pos=find_word_type(word_type_dict,'JJ'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " (';', ':'),\n",
       " ('fruit', 'CC'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('banana', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example to show the flaws in pos tagging\n",
    "seq = nltk.word_tokenize(\"Time flies like an arrow; fruit flies like a banana.\")\n",
    "nltk.pos_tag(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a pos_tagged sentence as function argument\n",
    "def syn_replace_choice(pos_tags : list):\n",
    "    replacable_tags = ['JJ', 'JJR', 'JJS','RB', 'RBR', 'RBS']\n",
    "    filtered_data = [item for item in pos_tags if item[1] in replacable_tags]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(word : str, pos : str, seed : int = 42) -> str:\n",
    "    synonyms = []\n",
    "    if pos == None:\n",
    "        return word\n",
    "    for syn in wn.synsets(word, pos = pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    random.seed(seed)\n",
    "    return random.choice(list(set(synonyms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_type(target_tag):\n",
    "    word_type_dict = {wn.ADJ : ['JJ', 'JJR', 'JJS'],wn.ADV : ['RB', 'RBR', 'RBS']}\n",
    "    for key, tag_list in word_type_dict.items():\n",
    "        if target_tag in tag_list:\n",
    "            return key\n",
    "    return None  # Tag not found in any list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the first sentence of the 2 sentences which are compared as an argument\n",
    "#returns a tuple of of the random word and pos tag\n",
    "#if used for syn replacement only certain words are allowed\n",
    "def get_random_word(seq : str, min_len : int = 3,seed : int = None, sep = '[SEP]', syn_replace : bool = False) -> tuple:\n",
    "    random.seed(seed)\n",
    "    seq = seq.replace(sep, '')\n",
    "    tokens = nltk.word_tokenize(seq)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    if syn_replace:\n",
    "        pos_tags = syn_replace_choice(pos_tags)\n",
    "    \n",
    "    pos_tags = list(set(pos_tags))\n",
    "    candidates = [word for word in pos_tags if len(word[0]) >= min_len]\n",
    "    if(candidates == []):\n",
    "        return None\n",
    "\n",
    "    return random.choice(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_syn_replacement(seq : str, sep : str = '[SEP]', seed : int = None, p : float = 0.3):\n",
    "    word = get_random_word(seq, sep = sep, syn_replace= True, seed = seed)\n",
    "    if word[0] != None:\n",
    "        seq = seq.replace(word[0],get_synonym(word[0], pos = find_word_type(word[1]), seed = seed))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes one random letter of the word\n",
    "#dont interchange first or last letter\n",
    "def replace_letter(word : str, seed : int = None)-> str:\n",
    "    random.seed(seed)\n",
    "    if len(word) <=2:\n",
    "        return word\n",
    "    idx = random.randint(1,len(word)-2)\n",
    "    mod_word = word[:idx] + random.choice(string.ascii_lowercase) + word[idx + 1:] \n",
    "    return mod_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_change_letter(seq : str, p : int = 0.3, sep :str = '[SEP]', seed = None):\n",
    "    seq = seq.split(sep)\n",
    "    word = get_random_word(seq[0])[0]\n",
    "    random.seed(seed)\n",
    "    if word == None or random.random() < p:\n",
    "        return seq[0] + sep + seq[1]\n",
    "    return seq[0].replace(word,replace_letter(word,seed = seed)) + sep + seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You try to forget everything good, and remember everything byd.\n",
      "you just try to forget everything good, and remember everything bad.\n",
      "You try to forget everything good, and remember everything bad.\n",
      "you simply try to forget everything good, and remember everything bad.\n"
     ]
    }
   ],
   "source": [
    "print(apply_change_letter(tada, sep = '\\n'))\n",
    "print(apply_syn_replacement(tada, sep = '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.rand((1,32))\n",
    "emb = torch.rand((32,265,786))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9915, 0.9858, 0.8797, 0.8593, 0.8284, 0.8278, 0.8225, 0.8007, 0.7588,\n",
       "         0.7584, 0.6318, 0.6290, 0.6231, 0.6166, 0.6037, 0.6014, 0.5955, 0.5313,\n",
       "         0.4501, 0.4356, 0.4002, 0.3959, 0.3741, 0.3024, 0.3004, 0.1767, 0.1639,\n",
       "         0.1283, 0.0794, 0.0556, 0.0404, 0.0093]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to select the 2 closes scores in the batch and return their indices within the batch\n",
    "#def find_min_score_diff(emb : torch.Tensor,scores : torch.Tensor):\n",
    "sorted, indices = torch.sort(scores, descending = True)\n",
    "eps = 0.1\n",
    "#if sorted[0][0] - sorted[0][1] < eps:\n",
    "sorted   \n",
    "#print(apply_interpol(torch.rand(1,265,786),sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([265, 786])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_score_diff(scores : torch.Tensor)-> (int,int):\n",
    "    differences = torch.abs(scores.view(-1, 1) - scores.view(1, -1))\n",
    "    differences = differences.masked_fill(torch.eye(scores.numel()) == 1, float('inf'))\n",
    "    min_indices = torch.argmin(differences)\n",
    "    row_index = min_indices // differences.size(0)\n",
    "    col_index = min_indices % differences.size(0)\n",
    "    return row_index.item(),col_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for performing linear interpolation between 2 embeddings and there score\n",
    "def apply_interpol(inputs : torch.Tensor, scores : torch.Tensor, alpha  : float = 0.5) -> (torch.Tensor,torch.Tensor):\n",
    "    idx_a,idx_b = get_lowest_score_diff(scores)\n",
    "    inputs[idx_b] = torch.lerp(inputs[idx_a], inputs[idx_b], alpha)\n",
    "    scores[0][idx_b] = torch.lerp(scores[0][idx_a],scores[0][idx_b],alpha)\n",
    "    return inputs,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#distilbert-base-uncased\n",
    "#facebook/bart-base\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "print(type(tokenizer).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 14721, 329, 4193, 1499, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [0, 42606, 4193, 1499, 2], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('Comzustion'))\n",
    "print(tokenizer('Combustion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You try to forget everything good, and remember everything bad.\\nyou just try to forget everything good, and remember everything bad.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv('/Users/lemarx/Documents/01_projects/SentencesRelatedness24/data/raw/eng_train.csv')\n",
    "\n",
    "#dataframe['input'] = dataframe.apply(lambda row : row['Text'].replace('\\n',tokenizer.sep_token),axis = 1)\n",
    "tada = dataframe['Text'].iloc[200]\n",
    "tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1185, 860, 7, 4309, 960, 205, 6, 8, 2145, 960, 1099, 4, 2, 6968, 95, 860, 7, 4309, 960, 205, 6, 8, 2145, 960, 1099, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_together = tokenizer(tada)\n",
    "trial_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 1185,  860,    7, 4309,  960,  205,    6,    8, 2145,  960, 1099,\n",
       "            4,    2,    2, 6968,   95,  860,    7, 4309,  960,  205,    6,    8,\n",
       "         2145,  960, 1099,    4,    2,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tada.split('\\n')\n",
    "trial_seperate = tokenizer(features[0],features[1], return_tensors='pt', truncation='longest_first', padding='max_length',max_length=265)\n",
    "trial_seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_together == trial_seperate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ENG-train-0056</td>\n",
       "      <td>Moffitt said the results need to be replicated...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>ENG-train-0265</td>\n",
       "      <td>the united states government and other nato me...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>ENG-train-0511</td>\n",
       "      <td>The University of Michigan released today a ne...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>ENG-train-0856</td>\n",
       "      <td>A digital signal processor is a specialized mi...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>ENG-train-0877</td>\n",
       "      <td>an institute for public policy research report...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>ENG-train-0962</td>\n",
       "      <td>Kyi, a U.N. envoy says, as Japan adds to growi...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>ENG-train-1051</td>\n",
       "      <td>china's council of agriculture opens a special...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>ENG-train-1160</td>\n",
       "      <td>Maddox, 87, cracked two ribs when he fell abou...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>ENG-train-2000</td>\n",
       "      <td>Testing in 2008 by PassMark Software found Nor...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>ENG-train-2027</td>\n",
       "      <td>Surfactant/cleansing/foaming TIPA-LAURYL SULFA...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>ENG-train-2554</td>\n",
       "      <td>Sergei Viktorovich Fedorov is a Russian profes...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>ENG-train-3030</td>\n",
       "      <td>A New Castle County woman has become the first...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>ENG-train-3921</td>\n",
       "      <td>Combination with epoprostenol The combination ...</td>\n",
       "      <td>0.34</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PairID                                               Text  \\\n",
       "56    ENG-train-0056  Moffitt said the results need to be replicated...   \n",
       "265   ENG-train-0265  the united states government and other nato me...   \n",
       "511   ENG-train-0511  The University of Michigan released today a ne...   \n",
       "856   ENG-train-0856  A digital signal processor is a specialized mi...   \n",
       "877   ENG-train-0877  an institute for public policy research report...   \n",
       "962   ENG-train-0962  Kyi, a U.N. envoy says, as Japan adds to growi...   \n",
       "1051  ENG-train-1051  china's council of agriculture opens a special...   \n",
       "1160  ENG-train-1160  Maddox, 87, cracked two ribs when he fell abou...   \n",
       "2000  ENG-train-2000  Testing in 2008 by PassMark Software found Nor...   \n",
       "2027  ENG-train-2027  Surfactant/cleansing/foaming TIPA-LAURYL SULFA...   \n",
       "2554  ENG-train-2554  Sergei Viktorovich Fedorov is a Russian profes...   \n",
       "3030  ENG-train-3030  A New Castle County woman has become the first...   \n",
       "3921  ENG-train-3921  Combination with epoprostenol The combination ...   \n",
       "\n",
       "      Score  Text_len  \n",
       "56     0.97       323  \n",
       "265    0.88       332  \n",
       "511    0.81       335  \n",
       "856    0.75       304  \n",
       "877    0.75       317  \n",
       "962    0.75       330  \n",
       "1051   0.72       505  \n",
       "1160   0.70       314  \n",
       "2000   0.57       312  \n",
       "2027   0.56       559  \n",
       "2554   0.50       342  \n",
       "3030   0.47       309  \n",
       "3921   0.34       301  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['Text_len'] = dataframe.apply(lambda row : len(row['Text']),axis = 1)\n",
    "dataframe.loc[dataframe['Text_len'] >= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import wordnet as wn\n",
    "\n",
    "def get_random_word(seq : str, min_len : int = 3,seed : int = None, sep = '\\n', syn_replace : bool = False) -> tuple:\n",
    "        #random.seed(seed)\n",
    "        seq = seq.replace(sep, '')\n",
    "        tokens = nltk.word_tokenize(seq)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        if syn_replace:\n",
    "            pos_tags = syn_replace_choice(pos_tags)\n",
    "\n",
    "        pos_tags = list(set(pos_tags))\n",
    "        candidates = [word for word in pos_tags if len(word[0]) >= min_len]\n",
    "        if(candidates == []):\n",
    "            return (None,None)\n",
    "        print(candidates)\n",
    "        return random.choice(candidates)\n",
    "\n",
    "#takes a pos_tagged sentence as function argument\n",
    "def syn_replace_choice(pos_tags : list):\n",
    "    replacable_tags = ['JJ', 'JJR', 'JJS','RB', 'RBR', 'RBS']\n",
    "    filtered_data = [item for item in pos_tags if item[1] in replacable_tags]\n",
    "    return filtered_data\n",
    "\n",
    "def get_synonym(word : str, pos : str, seed : int = None) -> str:\n",
    "    synonyms = []\n",
    "    if pos == None:\n",
    "        return word\n",
    "    for syn in wn.synsets(word, pos = pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    #random.seed(seed)\n",
    "    if len(synonyms) <= 1:\n",
    "        return word\n",
    "    return random.choice(list(set(synonyms)))\n",
    "\n",
    "def find_word_type(target_tag):\n",
    "    word_type_dict = {wn.ADJ : ['JJ', 'JJR', 'JJS'],wn.ADV : ['RB', 'RBR', 'RBS']}\n",
    "    for key, tag_list in word_type_dict.items():\n",
    "        if target_tag in tag_list:\n",
    "            print(target_tag,tag_list)\n",
    "            return key\n",
    "    return None  # Tag not found in any list technically not possible\n",
    "\n",
    "def apply_syn_replacement(batch : list, sep : str = '\\n', seed : int = 42, p : float = 1):\n",
    "    batch_aug = []\n",
    "    for seq in batch:\n",
    "        print(seq)\n",
    "        word = get_random_word(seq, sep = sep, syn_replace= True, seed = seed)\n",
    "        print(word)\n",
    "        if word[0] != None:\n",
    "            batch_aug += [seq.replace(word[0],get_synonym(word[0], pos = find_word_type(word[1]), seed = seed))]\n",
    "        else:\n",
    "            batch_aug += [seq]\n",
    "    return batch_aug\n",
    "\n",
    "\n",
    "#changes one random letter of the word\n",
    "#dont interchange first or last letter\n",
    "def replace_letter(word : str, seed : int = None)-> str:\n",
    "    #random.seed(seed)\n",
    "    if len(word) <=2:\n",
    "        return word\n",
    "    idx = random.randint(1,len(word)-2)\n",
    "    mod_word = word[:idx] + random.choice(string.ascii_lowercase) + word[idx + 1:]\n",
    "    return mod_word\n",
    "\n",
    "def apply_change_letter(batch : list, p : int = 0.3, sep :str = '\\n', seed = None):\n",
    "    batch_aug = []\n",
    "    for seq in batch:\n",
    "        seq = seq.split(sep)\n",
    "        word = get_random_word(seq[0])[0]\n",
    "        # random.seed(42)\n",
    "        if word == None or random.random() < p:\n",
    "            batch_aug +=  [seq[0] + sep + seq[1]]\n",
    "            continue\n",
    "        batch_aug += [seq[0].replace(word,replace_letter(word,seed = seed)) + sep + seq[1]]\n",
    "    return batch_aug\n",
    "\n",
    "def encode(batch):\n",
    "    batch_aug = apply_change_letter(batch['Text'])\n",
    "    tokenized = tokenizer(batch['Text'],return_tensors=\"pt\", padding=True,truncation=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   243,    14,  2594,     6,    95,  2999,     5, 10242,     4,\n",
      "         50118,  1594,    14,   655,  2594,     6,    95,  2999,     5, 10242,\n",
      "             4,     2,     1,     1,     1],\n",
      "        [    0,   250,   909,  2335,   878,   149,   514,     4, 50118,   250,\n",
      "           909,  2335,    16,   878,   149,   103,   514,     4,     2,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    0,   100,   348,    57,  6062,   627,  1445,  4091, 27993,    13,\n",
      "            47,     4, 50118,   100,   437,   546,    13,    47,    70,    81,\n",
      "             5,  4091, 27993,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/Users/lemarx/Documents/01_projects/SentencesRelatedness24/data/raw/eng_train.csv\")\n",
    "dataset.set_transform(encode)\n",
    "print(dataset['train'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you just try to forget everything good, and remember everything bad.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "help1,help2 =tada.split('\\n')\n",
    "help2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_aug_1 = []\n",
    "batch_aug_2 = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
