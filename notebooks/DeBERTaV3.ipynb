{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification,AutoTokenizer\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSimilarityModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=2e-5):\n",
    "        super(SentenceSimilarityModel, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-small')\n",
    "        self.deberta = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-v3-small',ignore_mismatched_sizes=True,num_labels=1)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        inputs = self.tokenizer(input_ids, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = self.deberta(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        return torch.sigmoid(outputs.logits)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        \n",
    "        # Assuming a binary classification task\n",
    "        loss = F.binary_cross_entropy_with_logits(torch.sigmoid(logits), labels.float())\n",
    "        spearman = spearmanr(torch.sigmoid(logits).detach().numpy(),labels.detach().numpy()).statistic\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"spearman\", spearman, on_step=True, on_epoch=True, prog_bar=True)\n",
    "       \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        logits = self(input_ids)\n",
    "        \n",
    "        # Assuming a binary classification task\n",
    "        loss = F.binary_cross_entropy_with_logits(torch.sigmoid(logits), labels.float())\n",
    "        \n",
    "        spearman = spearmanr(torch.sigmoid(logits).detach().numpy(),labels.detach().numpy()).statistic\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"spearman\",spearman)\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class str_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the features and target from the DataFrame\n",
    "        # Adjust this based on your DataFrame structure\n",
    "        features = self.dataframe['input'].loc[idx]\n",
    "        label = self.dataframe['Score'].loc[idx]\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32).unsqueeze(dim=0)  # Adjust the dtype as needed\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STR_DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-train-0000</td>\n",
       "      <td>It that happens, just pull the plug.\\nif that ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-train-0001</td>\n",
       "      <td>A black dog running through water.\\nA black do...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-train-0002</td>\n",
       "      <td>I've been searchingthe entire abbey for you.\\n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-train-0003</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-train-0004</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PairID                                               Text  Score\n",
       "0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0\n",
       "1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0\n",
       "2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0\n",
       "3  ENG-train-0003  If he is good looking and has a good personali...    1.0\n",
       "4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(os.path.join('..','data','raw','eng_train.csv'))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It that happens, just pull the plug.[SEP]if that ever happens, just pull the plug.'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = '[SEP]'\n",
    "test = train_data['Text'].loc[0].replace('\\n',sep)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-train-0000</td>\n",
       "      <td>It that happens, just pull the plug.\\nif that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It that happens, just pull the plug.[SEP]if th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-train-0001</td>\n",
       "      <td>A black dog running through water.\\nA black do...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A black dog running through water.[SEP]A black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-train-0002</td>\n",
       "      <td>I've been searchingthe entire abbey for you.\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've been searchingthe entire abbey for you.[S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-train-0003</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-train-0004</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PairID                                               Text  Score  \\\n",
       "0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0   \n",
       "1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0   \n",
       "2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0   \n",
       "3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n",
       "4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n",
       "\n",
       "                                               input  \n",
       "0  It that happens, just pull the plug.[SEP]if th...  \n",
       "1  A black dog running through water.[SEP]A black...  \n",
       "2  I've been searchingthe entire abbey for you.[S...  \n",
       "3  If he is good looking and has a good personali...  \n",
       "4  She does not hate you, she is just annoyed wit...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['input'] = train_data.apply(lambda row : row['Text'].replace('\\n',sep),axis = 1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/str23/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.q_bias', 'classifier.weight', 'deberta.encoder.layer.5.attention.self.v_bias', 'classifier.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized because the shapes did not match:\n",
      "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[0.4954]])\n"
     ]
    }
   ],
   "source": [
    "test = train_data['input'].loc[0]\n",
    "model = SentenceSimilarityModel()\n",
    "# token = model.tokenizer(test, return_tensors= 'pt')\n",
    "# input_ids = token['input_ids']\n",
    "# attention_mask = token['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    logits = model(test)\n",
    "\n",
    "\n",
    "print(\"Logits:\", logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_data['input'].loc[0]\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = str_dataset(train_df.reset_index(drop=True))\n",
    "val_dataset = str_dataset(val_df.reset_index(drop=True))\n",
    "\n",
    "str_datamodule = STR_DataModule(train_dataset=train_dataset, val_dataset= val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deberta-v2 to instantiate a model of type deberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.5.attention.self.q_bias', 'classifier.weight', 'deberta.encoder.layer.5.attention.self.v_bias', 'classifier.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.q_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized because the shapes did not match:\n",
      "- deberta.encoder.rel_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([1024, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/lemarx/anaconda3/envs/str23/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Missing logger folder: /Users/lemarx/Documents/01_projects/SentencesRelatedness24/notebooks/lightning_logs\n",
      "\n",
      "  | Name    | Type                             | Params\n",
      "-------------------------------------------------------------\n",
      "0 | deberta | DebertaForSequenceClassification | 149 M \n",
      "-------------------------------------------------------------\n",
      "149 M     Trainable params\n",
      "0         Non-trainable params\n",
      "149 M     Total params\n",
      "597.461   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/str23/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/str23/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  22%|██▏       | 246/1100 [01:39<05:46,  2.46it/s, v_num=0, train_loss_step=0.647, spearman_step=-.200]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/str23/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "model = SentenceSimilarityModel()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=2,accelerator='cpu')\n",
    "trainer.fit(model, datamodule=str_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
