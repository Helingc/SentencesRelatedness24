{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You try to forget everything good, and remember everything bad.\\nyou just try to forget everything good, and remember everything bad.'"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('/Users/lemarx/Documents/01_projects/SentencesRelatedness24/data/raw/eng_train.csv')\n",
    "tada = train_data['Text'].iloc[200]\n",
    "tada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lemarx/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/lemarx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21')]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = tada.split('\\n')[0]\n",
    "# seq = re.sub(r\"[^a-zA-Z0-9]+\", ' ', test)\n",
    "# seq = list(set(seq.split()))\n",
    "\n",
    "# candidates = [word for word in seq if len(word) >= 3]\n",
    "# #words = nltk.word_tokenize(seq)\n",
    "# pos_tags = nltk.pos_tag(candidates)\n",
    "\n",
    "\n",
    "# word,tag = pos_tags[0]\n",
    "\n",
    "# wn.synsets('good', pos=find_word_type(word_type_dict,'JJ'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " (';', ':'),\n",
       " ('fruit', 'CC'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('banana', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example to show the flaws in pos tagging\n",
    "seq = nltk.word_tokenize(\"Time flies like an arrow; fruit flies like a banana.\")\n",
    "nltk.pos_tag(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a pos_tagged sentence as function argument\n",
    "def syn_replace_choice(pos_tags : list):\n",
    "    replacable_tags = ['JJ', 'JJR', 'JJS','RB', 'RBR', 'RBS']\n",
    "    filtered_data = [item for item in pos_tags if item[1] in replacable_tags]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonym(word : str, pos : str, seed : int = 42) -> str:\n",
    "    synonyms = []\n",
    "    if pos == None:\n",
    "        return word\n",
    "    for syn in wn.synsets(word, pos = pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    random.seed(seed)\n",
    "    return random.choice(list(set(synonyms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_type(target_tag):\n",
    "    word_type_dict = {wn.ADJ : ['JJ', 'JJR', 'JJS'],wn.ADV : ['RB', 'RBR', 'RBS']}\n",
    "    for key, tag_list in word_type_dict.items():\n",
    "        if target_tag in tag_list:\n",
    "            return key\n",
    "    return None  # Tag not found in any list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes the first sentence of the 2 sentences which are compared as an argument\n",
    "#returns a tuple of of the random word and pos tag\n",
    "#if used for syn replacement only certain words are allowed\n",
    "def get_random_word(seq : str, min_len : int = 3,seed : int = None, sep = '[SEP]', syn_replace : bool = False) -> tuple:\n",
    "    random.seed(seed)\n",
    "    seq = seq.replace(sep, '')\n",
    "    tokens = nltk.word_tokenize(seq)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    if syn_replace:\n",
    "        pos_tags = syn_replace_choice(pos_tags)\n",
    "    \n",
    "    pos_tags = list(set(pos_tags))\n",
    "    candidates = [word for word in pos_tags if len(word[0]) >= min_len]\n",
    "    if(candidates == []):\n",
    "        return None\n",
    "\n",
    "    return random.choice(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_syn_replacement(seq : str, sep : str = '[SEP]', seed : int = None, p : float = 0.3):\n",
    "    word = get_random_word(seq, sep = sep, syn_replace= True, seed = seed)\n",
    "    if word[0] != None:\n",
    "        seq = seq.replace(word[0],get_synonym(word[0], pos = find_word_type(word[1]), seed = seed))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes one random letter of the word\n",
    "#dont interchange first or last letter\n",
    "def replace_letter(word : str, seed : int = None)-> str:\n",
    "    random.seed(seed)\n",
    "    if len(word) <=2:\n",
    "        return word\n",
    "    idx = random.randint(1,len(word)-2)\n",
    "    mod_word = word[:idx] + random.choice(string.ascii_lowercase) + word[idx + 1:] \n",
    "    return mod_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_change_letter(seq : str, p : int = 0.3, sep :str = '[SEP]', seed = None):\n",
    "    seq = seq.split(sep)\n",
    "    word = get_random_word(seq[0])[0]\n",
    "    random.seed(seed)\n",
    "    if word == None or random.random() < p:\n",
    "        return seq[0] + sep + seq[1]\n",
    "    return seq[0].replace(word,replace_letter(word,seed = seed)) + sep + seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You try to forget everything good, and remember everything byd.\n",
      "you just try to forget everything good, and remember everything bad.\n",
      "You try to forget everything good, and remember everything bad.\n",
      "you simply try to forget everything good, and remember everything bad.\n"
     ]
    }
   ],
   "source": [
    "print(apply_change_letter(tada, sep = '\\n'))\n",
    "print(apply_syn_replacement(tada, sep = '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.rand((1,32))\n",
    "emb = torch.rand((32,265,786))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9915, 0.9858, 0.8797, 0.8593, 0.8284, 0.8278, 0.8225, 0.8007, 0.7588,\n",
       "         0.7584, 0.6318, 0.6290, 0.6231, 0.6166, 0.6037, 0.6014, 0.5955, 0.5313,\n",
       "         0.4501, 0.4356, 0.4002, 0.3959, 0.3741, 0.3024, 0.3004, 0.1767, 0.1639,\n",
       "         0.1283, 0.0794, 0.0556, 0.0404, 0.0093]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to select the 2 closes scores in the batch and return their indices within the batch\n",
    "#def find_min_score_diff(emb : torch.Tensor,scores : torch.Tensor):\n",
    "sorted, indices = torch.sort(scores, descending = True)\n",
    "eps = 0.1\n",
    "#if sorted[0][0] - sorted[0][1] < eps:\n",
    "sorted   \n",
    "#print(apply_interpol(torch.rand(1,265,786),sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([265, 786])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowest_score_diff(scores : torch.Tensor)-> (int,int):\n",
    "    differences = torch.abs(scores.view(-1, 1) - scores.view(1, -1))\n",
    "    differences = differences.masked_fill(torch.eye(scores.numel()) == 1, float('inf'))\n",
    "    min_indices = torch.argmin(differences)\n",
    "    row_index = min_indices // differences.size(0)\n",
    "    col_index = min_indices % differences.size(0)\n",
    "    return row_index.item(),col_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for performing linear interpolation between 2 embeddings and there score\n",
    "def apply_interpol(inputs : torch.Tensor, scores : torch.Tensor, alpha  : float = 0.5) -> (torch.Tensor,torch.Tensor):\n",
    "    idx_a,idx_b = get_lowest_score_diff(scores)\n",
    "    inputs[idx_b] = torch.lerp(inputs[idx_a], inputs[idx_b], alpha)\n",
    "    scores[0][idx_b] = torch.lerp(scores[0][idx_a],scores[0][idx_b],alpha)\n",
    "    return inputs,scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7588)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0][29].item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_interpol(inputs = emb, scores = scores, alpha=0.5)[1].shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "str23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
