{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import spearmanr, pearsonr, stats\n",
    "from scipy import spatial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# For Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PATH = os.path.join(\"drive\", \"MyDrive\", \"LMU\", \"AppliedDL\", \"data\", \"raw\")\n",
    "    \n",
    "    !pip install lightning\n",
    "else:\n",
    "    PATH = os.path.join(\"..\", \"data\", \"raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(subset):\n",
    "\n",
    "    df_train = pd.read_csv(os.path.join(PATH, 'eng_train.csv'))\n",
    "    df_train[\"Split_Text\"] = df_train[\"Text\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "    df_train['Split_Text'] = df_train['Split_Text'].apply(lambda x: x.split(\"\\r\"))\n",
    "    df_train['Split_Text'] = df_train['Split_Text'].apply(lambda x: [re.sub(r\"[^a-zA-Z0-9]+\", ' ', k) for k in x])\n",
    "\n",
    "    df_train[\"sen_1\"] = df_train[\"Split_Text\"].apply(lambda x: x[0])\n",
    "    df_train[\"sen_2\"] = df_train[\"Split_Text\"].apply(lambda x: x[1])\n",
    "    df_train.drop([\"Split_Text\"], axis=1, inplace=True)\n",
    "    display(df_train.head())\n",
    "\n",
    "    if subset is not None:\n",
    "        df_train = df_train.sample(n=subset, random_state=42)\n",
    "\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "class Classifier(L.LightningModule):\n",
    "    def __init__(self, model_name):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=1\n",
    "        )\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        self.val_predictions = []\n",
    "        self.val_labels = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        predictions = outputs.logits.squeeze()\n",
    "\n",
    "        # Append predictions and labels to the lists\n",
    "        self.val_predictions.append(predictions)\n",
    "        self.val_labels.append(labels.squeeze())\n",
    "\n",
    "        return predictions, labels\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate predictions and labels at the end of each epoch\n",
    "        predictions = torch.cat(self.val_predictions)\n",
    "        labels = torch.cat(self.val_labels)\n",
    "\n",
    "        mse = self.loss_fn(predictions, labels)\n",
    "        spearman_corr, _ = spearmanr(\n",
    "            predictions.cpu().numpy(), labels.cpu().numpy()\n",
    "        )\n",
    "\n",
    "        # Log the metrics\n",
    "        self.log(\"val_loss\", mse, prog_bar=True)\n",
    "        self.log(\"val_spearman_corr\", spearman_corr, prog_bar=True)\n",
    "\n",
    "        # Clear the lists for the next epoch\n",
    "        self.val_predictions = []\n",
    "        self.val_labels = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=5e-5)\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "def prepare_torch_dataset(df, tokenizer):\n",
    "    tokenized = tokenizer(\n",
    "        df[\"sen_1\"].tolist(),\n",
    "        df[\"sen_2\"].tolist(),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(df[\"Score\"].tolist(), dtype=torch.float32).unsqueeze(1)\n",
    "    return TensorDataset(tokenized[\"input_ids\"], tokenized[\"attention_mask\"], labels)\n",
    "\n",
    "\n",
    "def run_cv(df, model_name, n_splits=5, epochs=3, batch_size=8, test_run=False):\n",
    "    if test_run:\n",
    "        n_splits = 2\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "    all_spearman_corrs = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "\n",
    "        print(f\"Fold {fold + 1}/{kf.get_n_splits()}\")\n",
    "\n",
    "        # Initialize the Lightning model\n",
    "        model = Classifier(model_name)\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=\"auto\", max_epochs=epochs, num_sanity_val_steps=0, fast_dev_run=test_run\n",
    "        )\n",
    "\n",
    "\n",
    "        # Split data into train and validation sets\n",
    "        train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "        train_data = prepare_torch_dataset(train_df, tokenizer)\n",
    "        val_data = prepare_torch_dataset(val_df, tokenizer)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=False)\n",
    "\n",
    "\n",
    "        # Train & Evaluate the model\n",
    "        trainer.fit(model, train_dataloader)\n",
    "        trainer.test(model, val_dataloader)\n",
    "\n",
    "\n",
    "        # Calculate and print the average Spearman correlation\n",
    "        average_spearman_corr = trainer.callback_metrics[\"val_spearman_corr\"].mean()\n",
    "        print(\n",
    "            f\"Average Spearman Correlation for Fold {fold + 1}: {average_spearman_corr}\"\n",
    "        )\n",
    "\n",
    "        all_spearman_corrs.append(average_spearman_corr)\n",
    "\n",
    "    # Calculate and print the overall average Spearman correlation\n",
    "    overall_average_spearman_corr = sum(all_spearman_corrs) / len(all_spearman_corrs)\n",
    "    print(\n",
    "        f\"Overall Average Spearman Correlation across all folds: {overall_average_spearman_corr}\"\n",
    "    )\n",
    "\n",
    "    return overall_average_spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation using CV\n",
    "\n",
    "Run with CUDA GPU acceleration\n",
    "- 8h with a fast CPU\n",
    "- XXX min on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PairID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>sen_1</th>\n",
       "      <th>sen_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENG-train-0000</td>\n",
       "      <td>It that happens, just pull the plug.\\r\\nif tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It that happens just pull the plug</td>\n",
       "      <td>if that ever happens just pull the plug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG-train-0001</td>\n",
       "      <td>A black dog running through water.\\r\\nA black ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A black dog running through water</td>\n",
       "      <td>A black dog is running through some water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG-train-0002</td>\n",
       "      <td>I've been searchingthe entire abbey for you.\\r...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I ve been searchingthe entire abbey for you</td>\n",
       "      <td>I m looking for you all over the abbey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG-train-0003</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>If he is good looking and has a good personali...</td>\n",
       "      <td>If he s good looking and a good personality h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENG-train-0004</td>\n",
       "      <td>She does not hate you, she is just annoyed wit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>She does not hate you she is just annoyed with...</td>\n",
       "      <td>She doesn t hate you she is just annoyed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PairID                                               Text  Score  \\\n",
       "0  ENG-train-0000  It that happens, just pull the plug.\\r\\nif tha...    1.0   \n",
       "1  ENG-train-0001  A black dog running through water.\\r\\nA black ...    1.0   \n",
       "2  ENG-train-0002  I've been searchingthe entire abbey for you.\\r...    1.0   \n",
       "3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n",
       "4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n",
       "\n",
       "                                               sen_1  \\\n",
       "0                It that happens just pull the plug    \n",
       "1                 A black dog running through water    \n",
       "2       I ve been searchingthe entire abbey for you    \n",
       "3  If he is good looking and has a good personali...   \n",
       "4  She does not hate you she is just annoyed with...   \n",
       "\n",
       "                                               sen_2  \n",
       "0           if that ever happens just pull the plug   \n",
       "1         A black dog is running through some water   \n",
       "2            I m looking for you all over the abbey   \n",
       "3   If he s good looking and a good personality h...  \n",
       "4          She doesn t hate you she is just annoyed   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dchro\\.conda\\envs\\AppliedDL\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-large and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dchro\\.conda\\envs\\AppliedDL\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name    | Type                        | Params\n",
      "--------------------------------------------------------\n",
      "0 | model   | T5ForSequenceClassification | 738 M \n",
      "1 | loss_fn | MSELoss                     | 0     \n",
      "--------------------------------------------------------\n",
      "738 M     Trainable params\n",
      "0         Non-trainable params\n",
      "738 M     Total params\n",
      "2,954.875 Total estimated model params size (MB)\n",
      "c:\\Users\\dchro\\.conda\\envs\\AppliedDL\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf38c6c6d6e4419970008b3a39b0098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n",
      "c:\\Users\\dchro\\.conda\\envs\\AppliedDL\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041968b95d814a849de3427b738f72c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dchro\\.conda\\envs\\AppliedDL\\lib\\site-packages\\scipy\\stats\\_stats_py.py:5445: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4772031903266907     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_spearman_corr     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4772031903266907    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_spearman_corr    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Spearman Correlation for Fold 1: nan\n",
      "Fold 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-large and are newly initialized: ['classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name    | Type                        | Params\n",
      "--------------------------------------------------------\n",
      "0 | model   | T5ForSequenceClassification | 738 M \n",
      "1 | loss_fn | MSELoss                     | 0     \n",
      "--------------------------------------------------------\n",
      "738 M     Trainable params\n",
      "0         Non-trainable params\n",
      "738 M     Total params\n",
      "2,954.875 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b70944a3524597991ee65329fb5f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be64c817ac5140a7ad9e5a717c76734c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4064527451992035     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_spearman_corr     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            nan            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4064527451992035    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_spearman_corr    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           nan           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Spearman Correlation for Fold 2: nan\n",
      "Overall Average Spearman Correlation across all folds: nan\n"
     ]
    }
   ],
   "source": [
    "df = get_data(subset=None) # Set to None to use the full dataset, set to 100 to prototype\n",
    "\n",
    "score = run_cv(\n",
    "    df,\n",
    "    model_name= \"t5-large\",\n",
    "    n_splits=5,\n",
    "    epochs=5,\n",
    "    batch_size=10,\n",
    "    test_run = True, # Set to True to run a test on one batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "- t5-small, 3 epochs - 0.57, 5 epochs - 0.75, 9 epochs - 0.8\n",
    "- t5-base, 3 epochs - 0.72, 9 epochs - 0.83"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
