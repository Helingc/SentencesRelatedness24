{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcZpv71Fnxdz"
      },
      "source": [
        "*  Using microsoft guidance library to force desired output format\n",
        "*  Using TheBloke C++ quantized LLAMA2 version to reduce RAM & vRAM requirements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yol-hzA9Yax5",
        "outputId": "8bfd4a76-bdf8-4365-cd4b-7cd8cbd6ec0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing llama-cpp-python...\n",
            "Installing guidance...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'CMAKE_ARGS' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: guidance in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (0.1.10)\n",
            "Requirement already satisfied: diskcache in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from guidance) (5.6.3)\n",
            "Requirement already satisfied: gptcache in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (0.1.43)\n",
            "Requirement already satisfied: openai>=1.0 in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (1.6.1)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from guidance) (3.11.0)\n",
            "Requirement already satisfied: tiktoken>=0.3 in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (0.5.2)\n",
            "Requirement already satisfied: msal in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (1.26.0)\n",
            "Requirement already satisfied: requests in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from guidance) (2.31.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from guidance) (1.26.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from guidance) (3.8.5)\n",
            "Requirement already satisfied: ordered-set in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (4.1.0)\n",
            "Requirement already satisfied: pyformlang in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from guidance) (1.0.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (1.8.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from openai>=1.0->guidance) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (2.4.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from openai>=1.0->guidance) (4.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from tiktoken>=0.3->guidance) (2023.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from requests->guidance) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from requests->guidance) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from requests->guidance) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from requests->guidance) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->guidance) (1.3.1)\n",
            "Requirement already satisfied: cachetools in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from gptcache->guidance) (5.3.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal->guidance) (2.8.0)\n",
            "Requirement already satisfied: cryptography<44,>=0.6 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from msal->guidance) (41.0.4)\n",
            "Requirement already satisfied: networkx in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from pyformlang->guidance) (3.1)\n",
            "Requirement already satisfied: pydot in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from pyformlang->guidance) (1.4.2)\n",
            "Requirement already satisfied: exceptiongroup in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai>=1.0->guidance) (1.1.3)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from cryptography<44,>=0.6->msal->guidance) (1.16.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\dchro\\.conda\\envs\\applieddl\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.0->guidance) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->guidance) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai>=1.0->guidance) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai>=1.0->guidance) (2.10.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai>=1.0->guidance) (0.4.6)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from pydot->pyformlang->guidance) (3.1.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\dchro\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.12->cryptography<44,>=0.6->msal->guidance) (2.21)\n",
            "Installing completed\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "def install_llama_cpp():\n",
        "\n",
        "    print(\"Installing llama-cpp-python...\")\n",
        "    try:\n",
        "        !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "    except:\n",
        "        try:\n",
        "            !pip install llama-cpp-python\n",
        "        except:\n",
        "            raise Exception(\"Could not install llama-cpp-python. Do you have C++ build tools installed?\")\n",
        "\n",
        "    print(\"Installing guidance...\")\n",
        "    try:\n",
        "        !pip install guidance\n",
        "    except:\n",
        "        raise Exception(\"Could not install guidance\")\n",
        "\n",
        "    print(\"Installing completed\")\n",
        "    return None\n",
        "\n",
        "install_llama_cpp()\n",
        "import guidance\n",
        "from guidance import models, gen, system, user, assistant\n",
        "\n",
        "def download_file_with_progress(url, save_path, filename):\n",
        "    \"\"\"\n",
        "    Download a file with progress indicator from a given URL\n",
        "\n",
        "    :param url: URL to the file\n",
        "    :param save_path: Path where the file will be saved\n",
        "    :param filename: Filename to save the downloaded content\n",
        "    \"\"\"\n",
        "    # Check if file already exists\n",
        "    full_path = os.path.join(save_path, filename)\n",
        "    if os.path.exists(full_path):\n",
        "        print(f\"The file {filename} already exists in {save_path}. Download skipped.\")\n",
        "        return\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024 * 1024 * 100 # 100 megabytes chunks\n",
        "    progress_bar_size = 50\n",
        "\n",
        "    print(f\"Starting download of {filename}\")\n",
        "    print(f\"Total download size: {total_size_in_bytes / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "    with open(full_path, 'wb') as file:\n",
        "        downloaded_size = 0\n",
        "        for data in response.iter_content(block_size):\n",
        "            downloaded_size += len(data)\n",
        "            file.write(data)\n",
        "            done = int(progress_bar_size * downloaded_size / total_size_in_bytes)\n",
        "            print(f\"\\r[{'â–ˆ' * done}{'.' * (progress_bar_size - done)}] {downloaded_size * 100 / total_size_in_bytes:.2f}%\", end='')\n",
        "    print(\"\\nDownload completed.\")\n",
        "\n",
        "class LLAMA:\n",
        "    def __init__(self, model_name) -> None:\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.initialize_model(self.model_name)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def initialize_model(self, model_name):\n",
        "        url_dict = {\"llama-2-7b-chat.Q2_K.gguf\": \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf?download=true\",\n",
        "                    \"llama-2-7b-chat.Q3_K_L.gguf\": \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q3_K_L.gguf?download=true\",\n",
        "                    \"llama-2-7b-chat.Q6_K.gguf\": \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q6_K.gguf?download=true\",\n",
        "                    \"llama-2-13b-chat.Q3_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q3_K_M.gguf?download=true\",\n",
        "                    \"llama-2-13b-chat.Q5_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true\",\n",
        "                    \"llama-2-13b-chat.Q6_K.gguf\": \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q6_K.gguf?download=true\",\n",
        "                    \"llama-2-70b-chat.Q5_K_M.gguf\": \"https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGUF/resolve/main/llama-2-70b-chat.Q5_K_M.gguf?download=true\"}\n",
        "\n",
        "        if model_name not in url_dict.keys():\n",
        "            raise Exception(\"Invalid model name. Valid model names are: \" + \", \".join(url_dict.keys()))\n",
        "\n",
        "        gguf_url = url_dict[model_name]\n",
        "        gguf_filename = model_name\n",
        "\n",
        "        if 'google.colab' in sys.modules:\n",
        "            save_path = \"\"\n",
        "        else:\n",
        "            save_path = os.path.join(\"..\", \"data\", \"pretrained_models\")\n",
        "\n",
        "        # Download the .gguf file with progress\n",
        "        download_file_with_progress(gguf_url, save_path, gguf_filename)\n",
        "\n",
        "        self.model = models.LlamaCpp(os.path.join(save_path, gguf_filename), n_gpu_layers=-1, n_ctx=4096)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def inference(self, sentence_pair, query = None):\n",
        "\n",
        "        # Limit the output to floats 0-1\n",
        "        regex_pattern = r\"0(\\.\\d+)?|1(\\.0+)?\"\n",
        "\n",
        "        if query is None:\n",
        "            query = f\"\"\"How semantically related are those two sentences on scale from 0 (least related) to 1 (most related): {sentence_pair}\"\"\"\n",
        "\n",
        "        output = self.model + f'''\\\n",
        "                Q: {query}\n",
        "                A: {gen('relatedness', regex=regex_pattern)}'''\n",
        "\n",
        "        return output\n",
        "\n",
        "# DATA\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PATH = os.path.join(\"drive\", \"MyDrive\", \"LMU\", \"AppliedDL\", \"data\", \"raw\")\n",
        "else:\n",
        "    PATH = os.path.join(\"..\", \"data\", \"raw\")\n",
        "\n",
        "def get_data(subset):\n",
        "\n",
        "    df_train = pd.read_csv(os.path.join(PATH, 'eng_train.csv'))\n",
        "    df_train[\"Split_Text\"] = df_train[\"Text\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
        "    df_train['Split_Text'] = df_train['Split_Text'].apply(lambda x: x.split(\"\\r\"))\n",
        "    df_train['Split_Text'] = df_train['Split_Text'].apply(lambda x: [re.sub(r\"[^a-zA-Z0-9]+\", ' ', k) for k in x])\n",
        "\n",
        "    df_train[\"sen_1\"] = df_train[\"Split_Text\"].apply(lambda x: x[0])\n",
        "    df_train[\"sen_2\"] = df_train[\"Split_Text\"].apply(lambda x: x[1])\n",
        "    df_train.drop([\"Split_Text\"], axis=1, inplace=True)\n",
        "    display(df_train.head())\n",
        "\n",
        "    if subset is not None:\n",
        "        df_train = df_train.sample(n=subset, random_state=42)\n",
        "\n",
        "    return df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z_sIFT5Yax6"
      },
      "source": [
        "### Initialize the model (smallest to biggest)\n",
        "- <12GB of RAM/vRAM\n",
        "    - llama-2-7b-chat.Q2_K.gguf\n",
        "    - llama-2-7b-chat.Q3_K_L.gguf\n",
        "    - llama-2-7b-chat.Q5_K_M.gguf\n",
        "- <24GB of RAM/vRAM\n",
        "    - llama-2-13b-chat.Q3_K_M.gguf\n",
        "    - llama-2-13b-chat.Q5_K_M.gguf\n",
        "    - llama-2-13b-chat.Q6_K.gguf\n",
        "- <64GB of RAM/vRAM\n",
        "    - llama-2-70b-chat.Q5_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "EzB1pduvYax6",
        "outputId": "c0bc6be1-7a7b-477e-d691-7ed4e066c1d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>                Q: How semantically related are those two sentences on scale from 0 (least related) to 1 (most related): [I like to eat apples. I like to eat oranges.]\n",
              "                A:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>0</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>5</span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'0.5'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the model\n",
        "LLAMA2 = LLAMA(\"llama-2-13b-chat.Q6_K.gguf\")\n",
        "# BLAS = 1 means there is GPU acceleration\n",
        "\n",
        "output = LLAMA2.inference(\"[I like to eat apples. I like to eat oranges.]\")\n",
        "output[\"relatedness\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFi4xrQMYax6"
      },
      "source": [
        "Get spearman on df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        },
        "id": "6Vdt20kJWkes",
        "outputId": "5e439b84-12f5-4ed0-eeea-545fa8f1dcbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>                Q: How semantically related are those two sentences on scale from 0 (least related) to 1 (most related): [Silence fell on the hilltop like a palpable thing ,  Here quite understandably silence fell under the lindens ]\n",
              "                A:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> </span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>0</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>5</span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spearman Correlation Coefficient: 0.29\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the whole train df ~ 1h\n",
        "df = get_data(subset=200)\n",
        "\n",
        "results = []\n",
        "for i in tqdm(range(len(df))):\n",
        "    sentence_pair = f'[{df.iloc[i, 3]}, {df.iloc[i, 4]}]'\n",
        "    score = df.iloc[i, 2]\n",
        "\n",
        "    output = LLAMA2.inference(sentence_pair)\n",
        "\n",
        "    # Append the results to the list\n",
        "    results.append((score, float(output[\"relatedness\"])))\n",
        "\n",
        "\n",
        "results = pd.DataFrame(results, columns = [\"Score\", \"Prediction\"])\n",
        "correlation, p_value = spearmanr(results[\"Score\"], results[\"Prediction\"])\n",
        "print(\"Spearman Correlation Coefficient:\", np.round(correlation, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
